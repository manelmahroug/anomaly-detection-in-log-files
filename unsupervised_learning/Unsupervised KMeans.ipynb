{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = '/home/thanuja/Dropbox/capstone/'\n",
    "APP_SYS_NAME = 'BGL'\n",
    "#APP_SYS_NAME = 'Thunderbird'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6388305",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = PROJECT_DIR + 'output/'\n",
    "RAW_DIR = PROJECT_DIR + 'raw_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa93594",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, confusion_matrix , precision_score, recall_score\n",
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "sys.path.append(PROJECT_DIR) # this is done to make the import of ad_feature_extraction work\n",
    "from ad_feature_extraction import parsers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2484a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(BASE_DIR + APP_SYS_NAME + '_all_params.csv')\n",
    "\n",
    "print(\"Input_data Shape:\",input_data.shape)\n",
    "print(input_data['label'].value_counts())\n",
    "\n",
    "sns.countplot(x=input_data['label'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49412064",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "input_data['label'] = le.fit_transform(input_data['label']) # false:0 and true:1\n",
    "\n",
    "X = input_data[['tfidf_text']].dropna()\n",
    "y = input_data.loc[X.index,'label'].values\n",
    "\n",
    "print(\"columns for the X file\"+ str(X.columns))\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebea2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do tfidf and kmeans on input_data and get homegeneity and completeness scores.\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X['tfidf_text'])\n",
    "print('tfidf shape', X_tfidf.shape)\n",
    "\n",
    "k = input_data['clusters'].max() + 1\n",
    "print('Using k = ', k)\n",
    "kmeans = KMeans(k)\n",
    "kmeans.fit(X_tfidf)\n",
    "\n",
    "pred = kmeans.predict(X_tfidf)\n",
    "labels = y\n",
    "\n",
    "print(\"_______________________________________________\")\n",
    "print(f\"Completeness Score: \\n {completeness_score(labels, pred)}\\n\")\n",
    "print(f\"Homogeneity Score: \\n {homogeneity_score(labels, pred)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c88711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count log lines within each cluster\n",
    "output = X.copy()\n",
    "output['cluster2'] = pred\n",
    "output['label'] = input_data.loc[X.index, 'label']\n",
    "print(output.groupby(['cluster2', 'label']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db709c6b",
   "metadata": {},
   "source": [
    "# Use label to determine which clusters are anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.groupby(['cluster2', 'label']).count())\n",
    "counts_by_label_df = output.groupby(['cluster2', 'label']).count().reset_index()\n",
    "pos_df = counts_by_label_df[counts_by_label_df['label'] == 1]\n",
    "neg_df = counts_by_label_df[counts_by_label_df['label'] == 0]\n",
    "anomaly_clusters = []\n",
    "normal_clusters = []\n",
    "for _, row in pos_df.iterrows():\n",
    "    cluster = row['cluster2']\n",
    "    pos = row['tfidf_text']\n",
    "    neg = neg_df[neg_df[\"cluster2\"] == cluster]['tfidf_text'].sum()\n",
    "    print(cluster, 'pos neg', pos, neg)\n",
    "    if pos > neg:\n",
    "        anomaly_clusters.append(cluster)\n",
    "\n",
    "for _, row in neg_df.iterrows():\n",
    "    cluster = row['cluster2']\n",
    "    neg = row['tfidf_text']\n",
    "    pos = pos_df[pos_df[\"cluster2\"] == cluster]['tfidf_text'].sum()\n",
    "    if neg >= pos:\n",
    "        normal_clusters.append(cluster)\n",
    "\n",
    "print('anomaly clusters', anomaly_clusters)\n",
    "print('normal clusters', normal_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5073f3",
   "metadata": {},
   "source": [
    "# Create summary report for clusters. Give example log line for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_file_name = BASE_DIR + APP_SYS_NAME + '_cluster_summary.csv'\n",
    "output_file = open(summary_file_name, 'w')\n",
    "with output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['is_anomaly', 'anomaly_count', 'normal_count',\n",
    "                     'cluster', 'pct_of_total',\n",
    "                     'most_similar_sample', 'most_similar_distance',\n",
    "                     'least_similar_sample', 'least_similar_distance'])\n",
    "    \n",
    "    def print_clusters(clusters, label):\n",
    "        num_rows = X.shape[0]\n",
    "        for ac in clusters:\n",
    "            indices = X.index[pred == ac]\n",
    "            rows_for_cluster = X_tfidf[pred == ac]\n",
    "            centroid = kmeans.cluster_centers_[ac]\n",
    "            least_distance = 0\n",
    "            most_similar = None\n",
    "            greatest_distance = 1\n",
    "            least_similar = None\n",
    "            for i in range(rows_for_cluster.shape[0]):\n",
    "                row = np.asarray(rows_for_cluster[i].todense()).reshape(-1)\n",
    "                distance = cosine(centroid, row)\n",
    "                if most_similar is None or least_distance > distance:\n",
    "                    least_distance = distance\n",
    "                    most_similar = i\n",
    "        \n",
    "                if least_similar is None or greatest_distance < distance:\n",
    "                    greatest_distance = distance\n",
    "                    least_similar = i\n",
    "                \n",
    "            pct = round(np.count_nonzero(pred == ac) / num_rows, 2)\n",
    "            writer.writerow([1 if label == 'anomaly' else 0,\n",
    "                             pos_df[pos_df[\"cluster2\"] == ac]['tfidf_text'].sum(),\n",
    "                             neg_df[neg_df[\"cluster2\"] == ac]['tfidf_text'].sum(),\n",
    "                             ac, pct,\n",
    "                             input_data.loc[indices[most_similar], 'text'], round(least_distance, 3),                  \n",
    "                             input_data.loc[indices[least_similar], 'text'], round(greatest_distance, 3)])\n",
    "    print_clusters(anomaly_clusters, 'anomaly')\n",
    "    print_clusters(normal_clusters,  ' normal')\n",
    "    \n",
    "summary_df = pd.read_csv(summary_file_name)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de13066",
   "metadata": {},
   "source": [
    "# Use TFIDF and KMeans from above to predict anomalies in original raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aac3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_label_map = np.zeros(k)\n",
    "for c in anomaly_clusters:\n",
    "    cluster_to_label_map[c] = 1\n",
    "    \n",
    "for c in normal_clusters:\n",
    "    cluster_to_label_map[c] = 0\n",
    "\n",
    "def process_raw_file(raw_filename, parser):\n",
    "    block_size = 100000 # entire dataset will not fit in memory, so do a block at a time\n",
    "    file_path = raw_filename.split('/')\n",
    "    with open(raw_filename, \"r\", encoding=\"utf8\", errors='ignore') as raw_file:\n",
    "        count = 0\n",
    "        fp_total = tp_total = fn_total = 0\n",
    "        row_block = []\n",
    "        y_block = np.zeros(block_size)\n",
    "        for line in raw_file:\n",
    "            line = line.rstrip('\\n')\n",
    "            epochts,text,is_anomaly,filename = parser(file_path, line)\n",
    "            y_block[count % block_size] = 1 if is_anomaly else 0\n",
    "            count += 1\n",
    "            row_block.append(text)\n",
    "            \n",
    "            if count % block_size == 0:\n",
    "                X_block = tfidf.transform(row_block)\n",
    "                block_clusters = kmeans.predict(X_block)\n",
    "                block_pred = np.zeros(block_size)\n",
    "                for i in range(block_clusters.shape[0]):\n",
    "                    block_pred[i] = cluster_to_label_map[block_clusters[i]]\n",
    "                fp = (block_pred == 1) & (y_block == 0)\n",
    "                tp = (block_pred == 1) & (y_block == 1)\n",
    "                fn = (block_pred == 0) & (y_block == 1)\n",
    "                fp_total += np.count_nonzero(fp)\n",
    "                tp_total += np.count_nonzero(tp)\n",
    "                fn_total += np.count_nonzero(fn)\n",
    "                print(count, fp_total, tp_total, fn_total)\n",
    "                row_block = []\n",
    "\n",
    "            # Remove this condition to process full file.\n",
    "            if count > 3000000: break\n",
    "        print(raw_filename, 'precision', tp_total / (fp_total + tp_total))\n",
    "        print(raw_filename, 'recall', tp_total / (fn_total + tp_total))\n",
    "\n",
    "process_raw_file(RAW_DIR + APP_SYS_NAME + '/' + APP_SYS_NAME + '.log', parsers[APP_SYS_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1af188",
   "metadata": {},
   "source": [
    "# Compute precision, recall, homogeneity, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc792a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def report(report_name, pred, labels):\n",
    "    label_pred = np.zeros(pred.size)\n",
    "    for i in range(pred.size):\n",
    "        label_pred[i] = cluster_to_label_map[pred[i]]\n",
    "    kmeans_report = pd.DataFrame(classification_report(labels, label_pred, output_dict=True))\n",
    "    print(\"{report_name} Result:\\n================================================\")        \n",
    "    print(f\"Accuracy Score: {accuracy_score(labels, label_pred) * 100:.2f}%\")\n",
    "    print(\"_______________________________________________\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{kmeans_report}\")\n",
    "    print(\"_______________________________________________\")\n",
    "    print(f\"Confusion Matrix: \\n {confusion_matrix(labels, label_pred)}\\n\")\n",
    "    \n",
    "    print(\"_______________________________________________\")\n",
    "    print(f\"Completeness Score: \\n {completeness_score(labels, pred)}\\n\")\n",
    "    print(\"_______________________________________________\")\n",
    "    print(f\"Homogeneity Score: \\n {homogeneity_score(labels, pred)}\\n\")\n",
    "    \n",
    "    print(\"_______________________________________________\")\n",
    "    print(f'Precision Score: \\n {precision_score(labels, label_pred)}')\n",
    "    print(\"_______________________________________________\")\n",
    "    print(f'Recall Score: \\n {recall_score(labels, label_pred)}')\n",
    "    print(\"_______________________________________________\")\n",
    "    print(f'Roc AUC Score: \\n {roc_auc_score(labels, label_pred)}')\n",
    "   \n",
    "    ConfusionMatrixDisplay.from_predictions(labels, label_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    PrecisionRecallDisplay.from_predictions(labels, label_pred, name=report_name)\n",
    "    plt.show()\n",
    "\n",
    "report(APP_SYS_NAME, pred, output.label.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
